{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
<<<<<<< HEAD
        "<a href=\"https://colab.research.google.com/github/imbiotech/skbtML/blob/main/k_pda_gmp_w_s.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
=======
        "<a href=\"https://colab.research.google.com/github/imbiotech/skbtML/blob/main/0.main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
>>>>>>> e2a00735482caa927fdc2c30600c22778b95291a
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-x9fR19bmt9I"
      },
      "source": [
        "**Machine Learning을 활용한 데이터 처리 및 분석 <br/>내부 적용 가능성에 대한 검토**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWVRjXMrnSQw"
      },
      "source": [
        "# 들어가기 전\n",
        "---\n",
        "\n",
        "**지난 10/06 한국 PDA에서 주최한 GMP Workshop에 참석했습니다.**  \n",
        "**`\"최근 AI를 활용한 의약품 제조 및 품질 관리\"`와 관련된 Workshop 이었으며, 그 중 이틀차 Python을 활용한 몇 가지 데이터 분석 실습 관련 세션에 참여하였습니다.**  \n",
        "**최근 업계에서는 `\"머신 러닝을 활용한 빅 데이터 처리 및 분석\"` 등으로 이야기 하는 분야로서 우리 회사 내에 얼마나 적용 가능성이 있을 지는 지속적으로 검증이 필요한 부분 중 하나이나,**  \n",
        "**우선 관련 내용을 공유 드리기 위해 아래와 같이 정리하여 드리니 테스트 해보시고 적용 가능 분야를 같이 고민해 주시면 감사하겠습니다.**  \n",
        "**아래 자료는 해당 세션의 강연자인 강원대 김화종 교수님의 자료와 각종 참고 데이터를 편집 및 재가공한 것입니다.**  \n",
        "**오류 지적 및 수정 요청에 대한 코멘트는 언제든지 부탁 드립니다.**\n",
        "\n",
        "#### 추가\n",
        "**MySUNI를 통한 관련 강좌를 수강하여 계속 내용 업데이트 중에 있습니다.**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtZW0VCcczmJ"
      },
      "source": [
        "# 목차\n",
        "\n",
        "- 개발 환경(Development Environment)\n",
        "  - 콜랩(Colab)이란?\n",
        "  - 파이썬(Python)이란?\n",
        "  - 쥬피터 노트북(Jupyter Notebook)이란?\n",
        "  > 코드 블럭 1: 라이브러리 설치\n",
        "\n",
        "- 머신 러닝(Machine Learning, ML)\n",
        "  - ML의 목적\n",
        "  - ML Model의 종류\n",
        "  - 데이터(x)\n",
        "  - ML의 성능 평가\n",
        "  - ML의 최적화기\n",
        "  - ML의 손실 함수\n",
        "\n",
        "- 파이썬과 내부 사례 데이터를 활용한 간단한 머신 러닝 프로그램 구현\n",
        "  - 사례 1. A 제품 수율 예측\n",
        "    - 데이터 준비(데이터 전처리, Data Preprocessing)\n",
        "      - 데이터 클리닝(Data Cleaning)\n",
        "      - 이상치 처리(Outlier handling)\n",
        "      - 데이터 변환(Data Transformation)\n",
        "      - 스케일링(Data Scaling)\n",
        "    - 모델 별 분석 및 성능 평가\n",
        "      - 회귀 모델(Regression Model)\n",
        "      - 결정 트리 모델(Decision Tree Model)\n",
        "      - 랜덤 포레스트 모델(Random Forest Model)\n",
        "      - 부스팅 모델(Boosting Model)\n",
        "      - 신경망 모델(Neural Network Model)\n",
        "        - 장단기 메모리(Long Short Term Memory)를 사용한 회귀 모델\n",
        "  - 사례 2. B 제품 수율 예측(케이스 작업 진행 중)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORb_vhUTyFm3"
      },
      "source": [
        "# 본문 1 - `개발 환경(Development Environment)`에 대하여"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNSEQ6HRyFm3"
      },
      "source": [
        "## `1.콜랩(Colab)`이란?\n",
        "- https://research.google.com/colaboratory/\n",
        "- 구글에서 지원하는 파이썬을 개발 및 테스트 하기 위한 가상 기기(Virtual machine)와 쥬피터 노트북의 기능을 담은 사이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlcX_7WCyFm3"
      },
      "source": [
        "## `2.파이썬(Python)`이란?\n",
        "- https://www.python.org/(Colab에서 직접 실행하므로 별도 설치는 불필요)\n",
        "- 프로그래밍 언어 중 한 종류로 C#, C++, Java 등 다른 프로그래밍 언어 대비 사용자 친화적인 체계를 가지고 있음\n",
        "- 태초부터 머신 러닝을 위한 프로그램으로 개발된 것이 아니므로 별도의 기능(라이브러리, Library)을 가져와서 아래 실습을 진행해야 함\n",
        "    - 본 자료에서는 이러한 오픈 소스들 중 `판다스(pandas), 넘파이(numpy), 맽플롯립(matplotlib), 사이킷런(sklearn), 케라스(keras)` 등의 라이브러리를 사용하여 머신 러닝을 구현할 예정\n",
        "> `라이브러리(Library)`란?<br/>\n",
        "일반적으로 각각의 언어에서 제공하는 기본적인 함수나 기능이 존재하는데 그 이상의 기능이 필요할 경우 유저가 직접 구현하여 사용함<br/>\n",
        "이렇게 특정 분야에 필요한 여러 기능들을 담아놓은 묶음(모듈, module)을 라이브러리라고 부름<br/>\n",
        "대부분의 라이브러리는 오픈 소스로 공개되어 있어 무료로 사용 가능하고 손쉽게 사용법을 찾아볼 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmQdTf94yFm4"
      },
      "source": [
        "## `3.쥬피터 노트북(Jupyter Notebook)`이란?\n",
        "- 쥬피터 노트북은 웹 사이트를 기반으로 한 개발 환경\n",
        "- 개발을 위한 별도의 프로그램(Integrated Development Environment, IDE) 설치없이 사용 가능\n",
        "- `파이썬 코드를 실행하기 위한 코드 블럭`과 `텍스트, 그래프, 그림과 같은 시각 자료를 추가할 수 있는 마크다운 언어를 탑재한 시스템`으로<br/> 본 페이지와 같이 사용자가 단순 코드 실행은 물론 필요한 내용을 담아 공유할 수 있음\n",
        "- 쥬피터 노트북에서 각 코드 블럭을 실행하기 위해서는 좌측의 재생 버튼을 눌러주면 됨(또는, 해당 코드 블럭 클릭 후 ctrl+Enter)\n",
        "    - 단, 일부 코드는 상위 코드 블럭을 실행해야만 데이터가 정상적으로 출력되므로 `\"코드 시작\" / \"코드 종료\"` 위치를 확인하여 코드 실행 필요\n",
        "\n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/howtostart.png?raw=true\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUPjFIGOyFm4"
      },
      "source": [
        "---\n",
        "### 코드블럭 1\n",
        "**_라이브러리 설치 코드 시작_**<br/>\n",
        "아래 코드는 본 자료의 실행에 필요한 라이브러리들을 가상 머신에 설치하는 코드<br/>\n",
        "다른 코드 실행 중 `\"No module named **\"`이라는 경고 문구가 출력될 경우 실행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kdAKE2ciyFm4"
      },
      "outputs": [],
      "source": [
        "pip install pandas numpy scikit-learn matplotlib lightgbm keras tensorflow seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHuUeETGyFm5"
      },
      "source": [
        "**_라이브러리 설치 코드 종료_**\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHYr24EVyFm5"
      },
      "source": [
        "# 본문 2 - `머신 러닝(Machine Learning, ML)`에 대하여"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwd07QiRhOi7"
      },
      "source": [
        "## `1.정의`\n",
        "- 컴퓨터가 데이터에서 유의미한 패턴과 통계적인 함수를 발견하여 행동 지침을 얻고  \n",
        "그에 따라 `수치를 예측(회귀, Regression) / 카테고리를 예측(분류, Classification) / 최적의 추천(Recommendation)` 등의 작업을 수행하는 것\n",
        "- 현재 `인공 지능(Artificial Intelligence, AI)`을 구현하는 가장 대표적인 방법으로 머신 러닝이 사용되고 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p-_T_MSqyFm6"
      },
      "source": [
        "## `2.목적`\n",
        "  | 대분류 | 소분류 |  Model 사용 목적 |\n",
        "  | :--: | :--: |  :--: |\n",
        "  | 예측(Prediction) | 회귀 예측(Regression Prediction) | 수치를 예측 |\n",
        "  | 예측(Prediction) | 분류 예측(Classification Prediction) | 대상의 카테고리를 예측 |\n",
        "  | 군집화(Clustering) | - | 대상이 속한 군집을 예측 |\n",
        "  | 설명(Description) | - | - |\n",
        "  | 추천(Recommendation) | - | - |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh2IxujcyFm6"
      },
      "source": [
        "## `3.ML 학습`\n",
        "- ML의 성능은 학습을 통해 개선될 수 있음\n",
        "\n",
        "| 학습의 종류 | 학습의 특징 |\n",
        "| :--: | :--: |\n",
        "| 지도 학습(Supervised Learing) | - 훈련 데이터 안에 예측해야 할 Target이 있는 경우에 사용되는 방법|\n",
        "|비지도 학습(Unsupervised Learning) | - 훈련 데이터 안에 예측해야 할 Target이 없으며<br/> 함수 모형을 만들지 않고 데이터의 패턴을 추출함<br/> - 데이터의 숨겨진 구조를 찾음 |\n",
        "|강화 학습(Reinforced Learning) | - 데이터가 스스로 정답을 찾기 위해서 환경과 상호작용<br/>- Target 값을 만들면서 훈련하고 해답에서 멀어질 수록 벌점이 부과되는 성질을 이용<br/>- 풀려고 하는 문제를 게임으로 간주하여 적용 |\n",
        "- 머신 러닝의 기본 동작 체계\n",
        "    \n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/Machine%20Learning%20Flow.png?raw=true\" align='left' width=\"700\">   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N51d6lqQyFm6"
      },
      "source": [
        "## `4.Model`\n",
        "- 입력 데이터(X)로부터 잘 예측된 정답(label 또는 target, y)를 얻는 각각의 소프트웨어(코드)를 모델이라고 함\n",
        "  - 정답을 잘 예측하기 위해서 각 모델을 구성하는 파라미터의 최적치를 찾아야 함\n",
        "- 모델에는 다음과 같이 다양한 종류가 있음.\n",
        "<br/> 각 모델에 대한 정보 및 예시를 한 장에 모두 담기는 어려워 따로 링크로 연결함\n",
        "  1. 결정 트리 모델(Decision Tree Model)\n",
        "  <br/> https://colab.research.google.com/github/imbiotech/skbtML/blob/main/4-1.Decision_Tree_Model.ipynb\n",
        "  2. 회귀 모델(Regression Model)\n",
        "  <br/> https://colab.research.google.com/github/imbiotech/skbtML/blob/main/4-2.Regression_Model.ipynb\n",
        "      - 선형 회귀 모델(Linear Regression Model)\n",
        "      - 로지스틱 회귀(Logistic Regression)\n",
        "  3. 군집 모델(Clustering Model)\n",
        "  <br/> https://colab.research.google.com/github/imbiotech/skbtML/blob/main/4-3.Clustering_Model.ipynb\n",
        "  4. KNN 모델(K-Nearest Neighbor Model)\n",
        "  <br/> https://colab.research.google.com/github/imbiotech/skbtML/blob/main/4-4.KNN_Model.ipynb\n",
        "\n",
        "  - SVM\n",
        "  - 랜덤 포레스트(Random Forest)\n",
        "  - 베이시언  \n",
        "  - 딥러닝 모델(MLP, CNN, RNN 등)(Deep Learning Model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6H_u3vmyFm6"
      },
      "source": [
        "### `4-1.데이터(x)와 결과(y)`\n",
        "- 머신 러닝은 학습에 사용하는 데이터가 많으면 많을 수록 예측 및 추천 성능이 향상됨\n",
        "- 단, 머신 러닝의 성능을 높이기 위해서는 원시 데이터(Raw Data)로부터<br/> 적절한 학습 및 검증 데이터를 만들기 위한 데이터 전처리(Data Preprocessing) 과정이 필요함\n",
        "- 탐색적 데이터 분석(Exploratory Data Analysis, EDA): ML Model을 만들기 위한 전단계,<br/> 특성을 추출하거나 데이터로서의 영감을 얻거나 전반적인 패턴을 알기 위해 통계적인 분석을 시도하는 것<br/> EDA를 통해 사용할 알고리즘을 결정하게 됨\n",
        "\n",
        "| 데이터의 종류 | 데이터의 특징 |\n",
        "| :--: | :--: |\n",
        "| 훈련 데이터(Training Set) | 모델을 학습하기 위한 데이터 |\n",
        "| 검증 데이터(Validation Set) | 학습 시킨 모델의 정확도를 검증하기 위한 데이터<br/> 반드시 훈련 데이터와는 다른 데이터여야 함 |\n",
        "| 정형 데이터(Structured Data) | 테이블 형태의 데이터 |\n",
        "| 비정형 데이터(Unstructured Data) | 이미지, 텍스트, 영상, 센서 데이터 등<br/>정형화되어 있지 않은 형태의 데이터 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50n4WLtCyFm6"
      },
      "source": [
<<<<<<< HEAD
=======
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvX6UpNLyFm6"
      },
      "source": [
>>>>>>> e2a00735482caa927fdc2c30600c22778b95291a
        "## `5.데이터 전처리(Data Preprocessing)`\n",
        "- ML 모델 구현을 위해 원시 데이터(Raw Data)를 그대로 사용할 경우 원하는 모델을 구현할 수 없거나, 구현하더라도 정확도가 떨어질 수 있음\n",
        "- 이러한 오류를 방지하기 위해 데이터의 정제 과정이 필요하고 이러한 과정을 데이터 전처리라고 부름\n",
        "- 데이터를 처리하는 방법에는 다음과 같은 것들이 존재함\n",
        "    1. 데이터 랭글링(Data Wrangling, 데이터 먼징, Data Munging)\n",
        "    2. 데이터 클리닝(Data Cleaning)\n",
        "    3. 이상치 처리(Outlier Handling):\n",
        "    4. 데이터 변환(Data Transformation)\n",
        "    5. 스케일링(Scaling)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFyerygByFm6"
      },
      "source": [
        "### `5-2. 데이터 클리닝(Data Cleaning)`\n",
        "- 데이터가 없거나(결측치, Missing Data) 틀린 값(Error)이 포함되어 있는 데이터는 다음 방법으로 처리함\n",
        "    - 해당 값이 포함된 샘플을 버린다.\n",
        "    - 해당 값을 적절한 값으로 대체한다.\n",
        "    - 해당 값을 그대로 두고 다음 분석 단계에서 처리한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cLH4hY5yFm6"
      },
      "source": [
        "### `5-3.이상치 처리(Outlier Handling)`\n",
        "- 데이터의 경향성에서 벗어나 있는 데이터는 분석의 정확도를 떨어뜨리므로 다음 방법으로 처리함\n",
        "- 이상치 탐지 방법\n",
        "    - 단변수 일 때\n",
        "        - 표준 편차가 n 이상인 값을 처리한다.(n의 값은 분석하는 사람이 적절히 정한다.)\n",
        "        - 실제 데이터를 통해 이상치를 파악한다.\n",
        "    - 다변수 일 때\n",
        "        - 주성분 분석(Pricipal Component Analysis, PCA)을 통해 찾는다.\n",
        "\n",
        "`상자 수염 그림의 이상치`\n",
        "1. 사분위수(Quartile, Q) 확인\n",
        "    > 사분위수란? 전체 데이터를 4등분 하는 위치에 있는 데이터\n",
        "    <br/>Q3 = 전체 데이터의 상위 25% 데이터\n",
        "    <br/>Q2 = 전체 데이터의 상위 50% 데이터\n",
        "    <br/>Q1 = 전체 데이터의 상위 75% 데이터\n",
        "2. 사분위수 범위 확인(Interquartile Range, IQR)\n",
        "    > 사분위수 범위란? 전체 데이터 중 중앙 50% 데이터의 구간\n",
        "    <br/>IQR = Q3 - Q1\n",
        "3. 이상치 한계 확인\n",
        "    > 이상치 한계란? 아래의 범위 양 끝을 이상치 한계로 정의하며, 구간을 벗어나면 이상치로 처리\n",
        "    <br/>하위 이상치 한계(Q1 - 1.5 * IQR) ~ 상위 이상치 한계(Q3 + 1.5 * IQR)\n",
        "4. 해당하는 이상치를 제거\n",
        "    \n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/Outlier.png?raw=true\" align='left' width=700>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyJccKODyFm6"
      },
      "source": [
        "### `5-4.데이터 변환(Data Transformation)`\n",
        "- 데이터의 상관 관계를 명확히 하기 위해서 데이터를 특정 형태로 변환하여 분석하는 방법\n",
        "- 다음과 같은 변환을 사용함\n",
        "\n",
        "| 변환 방식 | 내용 |\n",
        "| :--: | :--: |\n",
        "| 카테고리 인코딩(Category Encoding)|하나의 숫자로 표현되어 있지 않은 데이터를 하나의 숫자로 변환하여 분석하는 방법<br/>예) 봄은 1, 여름은 2, 가을은 3, 겨울은 4로 변환하여 분석<br/>연령대나 옷 사이즈 등 연속형 변수를 처리할 수도 있음<br/> 예) 10대는 1, 20대는 2, 30대는 3으로 치환|\n",
        "| 원 핫 인코딩(One-Hot Encoding)|카테고리 인코딩의 일종으로 <br/>국적, 성별, 계절 등 카테고리를 구분하는 변수들을 변환하는 방법|\n",
        "| 로그 변환(Logarithm Transform)|x 대신 log(x)를 사용|\n",
        "|역수 변환(Reciprocal Transform)| x 대신 1/x를 사용|\n",
        "|다차항 변환(Polynomial trnansformation)| x 대신 x^2 이나 x*y를 사용|\n",
        "|비율값 사용(Rate Transform)| x 대신 x/y를 사용|\n",
        "|새로운 값 정의(New Definition)| 특정 값 체계를 만들어서 변환<br/> 예) BMI = 몸무게 / 키**2|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYuZn8b8yFm7"
      },
      "source": [
        "### `5-5.스케일링(Scaling)`\n",
        "- 여러 변수의 범위가 서로 다른 경우 분석에서 동일한 비중으로 반영하기 위해서는 동일한 범위를 갖도록 변환해야 함\n",
        "    - Scaling 과정을 거치지 않을 경우에는 데이터 가중치 측면에서 수치가 작은 변수는 과대평가되고 수치가 큰 변수는 과소평가되는 현상이 발생함\n",
        "- 다양한 스케일링 방식이 존재하며 일반적으로 평균을 0, 표준 편차가 1이 되도록 변환하는 표준 스케일링(Standard Scaling)을 가장 많이 사용함\n",
        "- 값의 최소치와 최대치가 정해진 경우는 최소치를 0, 최대치를 1로 변환하는 min-max Scaling도 자주 사용함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8N5j_NwnvCq"
      },
      "source": [
        "## `6.성능 평가`\n",
        "- 작성된 머신 러닝 모델이 원하는 작업을 잘 수행하는지 평가하는 값\n",
        "- 주요 평가 척도\n",
        "  - 회귀 모델에서의 R-sqared(R^2)\n",
        "  - 분류 모델에서의 정확도(Accuracy), 정밀도(Precision), 리콜(Recall), f-1 점수, ROC-AUC 등"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybMVJQtCnwX-"
      },
      "source": [
        "## `7.최적화기`\n",
        "- 학습을 통해 모델 파라미터를 최적의 값으로 수렴시키는 알고리즘\n",
        "- 일반적으로 경사 하강법(Gradient descent, GD)이 사용됨"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62hfziqXnxMo"
      },
      "source": [
        "## `8.손실 함수(Loss function)`\n",
        "- 최적화기를 통해 손실 함수를 최소화하는 방향을 추구\n",
        "- 손실 함수를 통해 최적화기의 동작이 잘 이뤄지는지를 평가할 수 있음\n",
        "- 손실 함수의 종류\n",
        "  - 회귀 모델에서의 평균 제곱 오차(Mean square error, MSE)\n",
        "  - 분류 모델에서의 교차 엔트로피(Cross entropy, CE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-N9FSqbByFm7"
      },
      "source": [
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEbRV9_9tiZg"
      },
      "source": [
        "# 파이썬과 내부 사례 데이터를 활용한 간단한 머신 러닝 프로그램 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4H6fmYP5yFm7"
      },
      "source": [
        "## 사례 1. A 제품 수율 예측\n",
        "- 대상 제품 및 공정: A 제품의 a 공정 및 b 공정\n",
        "- 분석 개요: 공정 중 몇 가지 파라미터 p1, p2... 의 변화에 따른 수율 영향성을 평가하고, 수율을 예측"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1JbUU6-yFm7"
      },
      "source": [
        "---\n",
        "#### 코드블럭11\n",
        "**KNN 예시 코드 1 시작**\n",
        "\n",
        "이상형 데이터에 대한 KNN Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04q_Sliwelrj"
      },
      "source": [
        "### 데이터 준비\n",
        "- 본 케이스 스터디에서 다룰 데이터는 2가지 테이블로 다음과 같음\n",
        "  - 배치 번호, 공정, 기기 번호, 작업 시간의 데이터를 담은 테이블\n",
        "  - 계측 시간 및 계측 번호에 따른 계측값 데이터를 담은 테이블\n",
        "- 데이터는 WMS와 BAS를 통해 추출하였으며, 각 데이터 마다 적절한 1 차 sorting을 통해 불필요한 데이터를 정제한 상태임\n",
        "\n",
        "- 이 데이터를 pandas 라이브러리에서 제공하는 read_csv() 함수를 사용하여 읽어온 후 분석에 사용할 예정\n",
        "- pandas의 read_csv() 함수는 csv 파일의 데이터를 읽어와 데이터 프레임(Dataframe, DF)의 형태로 출력함\n",
        "  - DF는 행과 열로 구성된 2 차원 테이블 구조의 데이터\n",
        "  - 엑셀과 비슷한 기능을 수행할 수 있음"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLUKQetWpYa6"
      },
      "source": [
        "#### CSV란?\n",
        "- 쉼표(,)로 구분된 데이터 값(Comma-Separated Values)이 담긴 파일 형식\n",
        "- 단순한 텍스트 형식의 데이터 값으로 웹 페이지, 메모장, 엑셀 등등 어느 프로그램으로도 읽어올 수 있는 장점이 있음\n",
        "- 꼭 쉼표가 아니더라도 공백( )이나 세로 선(|) 등 특정 기호로 나눠져 있는 모든 데이터 파일을 의미"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWBRMrgflzAK"
      },
      "source": [
        "### 실행 코드\n",
        "\n",
        "---\n",
        "아래에 보이는 각각의 코드 블록은 순차적으로 실행하는 방식\n",
        "\n",
        "위쪽의 코드 블록을 실행하지 않으면 아래쪽을 실행하는 과정에서 오류가 발생할 수 있음\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIMgPoT3yFm7"
      },
      "outputs": [],
      "source": [
        "# 데이터프레임 기능을 사용하기 위한 pandas 라이브러리를 pd라는 이름으로 호출\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "# ipc_data의 주소 설정 및 ipc_df라는 이름으로 변수 선언\n",
        "ipc_data_address = \"https://raw.githubusercontent.com/imbiotech/skbtML/main/ipc_data.csv\"\n",
        "ipc_df_origin = pd.read_csv(ipc_data_address)\n",
        "\n",
        "\n",
        "# 데이터 확인\n",
        "ipc_df_origin[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOnwyehryFm8"
      },
      "outputs": [],
      "source": [
        "# Real Column과 Exp Column을 사용하여 Real Column 생성 후 기존 데이터 삭제\n",
        "if \"Real\" in ipc_df_origin.columns:\n",
        "    ipc_df_origin[\"Result\"] = round(ipc_df_origin[\"Real\"]/ipc_df_origin[\"Exp\"]*100,2)\n",
        "    ipc_df_origin.drop([\"Exp\",\"Real\"],axis=1,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_uFI1vMyFm8"
      },
      "outputs": [],
      "source": [
        "# Training Set과 Validation Set을 분리\n",
        "ipc_df_trn = ipc_df_origin[:112]\n",
        "ipc_df_val = ipc_df_origin[112:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQgKRSciyFm8"
      },
      "outputs": [],
      "source": [
        "# Training Set 데이터 확인\n",
        "ipc_df_trn[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irzXKc9CyFm8"
      },
      "outputs": [],
      "source": [
        "# Validation Set 데이터 확인\n",
        "ipc_df_val[:3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHxm4O8ByFm8"
      },
      "outputs": [],
      "source": [
        "# 특성 컬럼 지정, 해당 컬럼에 있는 값들이 데이터(X)로 사용됨\n",
        "features = ipc_df_trn.columns[1:7]\n",
        "\n",
        "\n",
        "# 데이터 전처리를 진행하기 전에 훈련 데이터(x)와 훈련 타겟(y) 사이의 상관 계수를 확인\n",
        "# 확인 시 오름 차순 정렬하고, 소수점 두 자리까지만 표기\n",
        "ipc_df_trn[features].corrwith(ipc_df_trn[\"Result\"]).sort_values(ascending=True).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MqSGZLVkyFm8"
      },
      "source": [
        "* Correlation의 절대값이 0에 가까울 수록 상관도가 낮은 값\n",
        "* Correlation의 절대값이 1에 가까울 수록 상관도가 높은 값\n",
        "    - 1에 가까울 수록 양의 상관 관계\n",
        "    - -1에 가까울 수록 음의 상관 관계\n",
        "\n",
        "- 다만 현재 데이터는 별도의 전처리를 거치지 않았기 때문에 상관 수치에 오류가 있을 수 있음\n",
        "- 전처리를 거친 후 다시 분석"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IsNLSMVtyFm-"
      },
      "outputs": [],
      "source": [
        "# 각 feature마다 별도의 상자 수염 그림을 그리는 함수, y 축은 자동 조정\n",
        "def boxplot_features(features,dataframe):\n",
        "    for feature in features:\n",
        "        dataframe.boxplot(column=feature,figsize=(5,5))\n",
        "        plt.title(feature)\n",
        "        plt.ylabel(feature)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kNyT4I1yFm_"
      },
      "outputs": [],
      "source": [
        "# ipc_df_trn 내부 데이터 중 이상치(outlier)를 확인\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "boxplot_features(features,ipc_df_trn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OuwgdvBMyFm_"
      },
      "outputs": [],
      "source": [
        "# 이상치 제거\n",
        "Q1 = ipc_df_trn[features].quantile(0.25)\n",
        "Q3 = ipc_df_trn[features].quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "ipc_df_trn = ipc_df_trn[~((ipc_df_trn[features] <(Q1 - 1.5 * IQR)) |(ipc_df_trn[features] >(Q3 + 1.5 * IQR))).any(axis=1)]\n",
        "# ipc_df_trn[features] <(Q1 - 1.5 * IQR) : Q1 - 1.5 * IQR 보다 작은 값들을 True로 반환\n",
        "# ipc_df_trn[features] >(Q3 + 1.5 * IQR) : Q3 + 1.5 * IQR 보다 큰 값들을 True로 반환\n",
        "#(ipc_df_trn[features] <(Q1 - 1.5 * IQR)) |(ipc_df_trn[features] >(Q3 + 1.5 * IQR)) : 두 조건 중 하나라도 True인 값들을 True로 반환\n",
        "# ~((ipc_df_trn[features] <(Q1 - 1.5 * IQR)) |(ipc_df_trn[features] >(Q3 + 1.5 * IQR))) : 두 조건 중 하나라도 True인 값들을 False로 반환\n",
        "# ipc_df_trn[~((ipc_df_trn[features] <(Q1 - 1.5 * IQR)) |(ipc_df_trn[features] >(Q3 + 1.5 * IQR)))] : 두 조건 중 하나라도 True인 행들을 제거\n",
        "\n",
        "\n",
        "# 이상치 제거 후 데이터 확인\n",
        "boxplot_features(features,ipc_df_trn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esNgBoglyFm_"
      },
      "source": [
        "* 각 상자 수염 그림을 확인했을 때 이상치들이 존재하고 있으며 데이터 클리닝을 위해 해당 이상치 데이터를 버림"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enMOX9gtyFm_"
      },
      "outputs": [],
      "source": [
        "# 상관 수치 재분석\n",
        "ipc_df_trn[features].corrwith(ipc_df_trn[\"Result\"]).sort_values(ascending=True).round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elTE9wiwyFm_"
      },
      "source": [
        "▼ 기존 데이터\n",
        "\n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/Correlation_Original.png?raw=true\" align='left'>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgvW3T5TyFm_"
      },
      "source": [
        "- 이상치 제거 전 데이터의 상관 관계 수치 대비 변동이 존재\n",
        "- p3, p4, p1 대비 p2, p5, p6는 상관 관계가 거의 없는 것으로 판단됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O6mAROlyFm_"
      },
      "outputs": [],
      "source": [
        "# plot 상태 변화를 확인하기 위해 ipc_df_trn을 복제\n",
        "ipc_df_trn_copy = ipc_df_trn.copy()\n",
        "\n",
        "\n",
        "# Result의 순서대로 나열하여 그래프를 그리기 위해 Result를 index로 설정\n",
        "if ipc_df_trn_copy.index.name != \"Result\":\n",
        "    ipc_df_trn_copy.set_index(\"Result\",inplace=True)\n",
        "    ipc_df_trn_copy.sort_index(ascending=True,inplace=True)\n",
        "\n",
        "\n",
        "# 각 데이터(x) vs 타겟(y) 에 대한 그래프 작성\n",
        "for f in features:\n",
        "    plt.figure(figsize=(10, 3))\n",
        "    ipc_df_trn_copy[f].plot()\n",
        "    plt.ylabel(f)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# 사용이 끝난 변수 삭제\n",
        "del ipc_df_trn_copy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPUIWHpnyFm_"
      },
      "source": [
        "- StandardScaler 모듈의 fit_transfer / fit / transfer함수를 사용하면 다음과 같은 작업이 진행됨\n",
        "    - fit: 각 컬럼의 데이터를 토대로 평균과 분산을 구하고 저장함\n",
        "    - transfer: 각 컬럼의 평균과 분산 데이터를 통해 데이터를 Standard Scaling 함\n",
        "\n",
        "- Training Set에서 Fit을 통해 평균과 분산을 구하고 해당 값으로 Training Set 및 Validation Set에 대한 Transform을 진행해야 함\n",
        "    - Fit: Training Set에 대해서만 진행\n",
        "    - Transform: Training Set에 대한 평균과 분산으로 두 개 Set에 모두 적용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RBtx7iyByFnA"
      },
      "outputs": [],
      "source": [
        "# Standard Scaling을 적용하기 위해 scikit-learn 라이브러리의 preprocessing 모듈에서 StandardScaler 함수를 호출\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "S_Scaler = StandardScaler()\n",
        "\n",
        "\n",
        "# Scaling 전 Training Set 확인\n",
        "print(\"Scaling 진행 전 Training Set\")\n",
        "print(ipc_df_trn[features][:3])\n",
        "\n",
        "\n",
        "# Scaling 및 Scaling 후 Training Set 확인\n",
        "ipc_df_trn[features] = S_Scaler.fit_transform(ipc_df_trn[features])\n",
        "print(\"\\nScaling 진행 후 Training Set\")\n",
        "print(ipc_df_trn[features][:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_rD9uEDyFnA"
      },
      "outputs": [],
      "source": [
        "# Scaling 전 Validation Set 확인\n",
        "print(\"Scaling 진행 전  Validation Set\")\n",
        "print(ipc_df_val[features][:3])\n",
        "\n",
        "\n",
        "# Scaling 및 Scaling 후 Validation Set 확인\n",
        "ipc_df_val[features] = S_Scaler.transform(ipc_df_val[features])\n",
        "print(\"\\nScaling 진행 후 Validation Set\")\n",
        "print(ipc_df_val[features][:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQhr4G96yFnA"
      },
      "source": [
        "- 검증 과정에서는 다음과 같은 절차를 진행\n",
        "    1. Training Set의 p1~p6(x_train)와 Result(y_train)사이의 관계를 통해 ML Model의 훈련을 진행한다.\n",
        "    2. 훈련이 끝난 ML Model을 사용하여 Validation Set의 p1~p6(x_val) 데이터로부터 예측 결과(y_pred)를 추출한다.\n",
        "    3. Validation Set의 실제 결과(y_val)와 예측 결과(y_pred)를 사용하여 Model의 정확도를 평가한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhkiSAYqyFnA"
      },
      "source": [
        "#### 회귀 모델(Regression Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiiYAgIzyFnA"
      },
      "outputs": [],
      "source": [
        "# 데이터 전처리 완료 및 모델 훈련 시작\n",
        "# 훈련 데이터(x_train) 및 훈련 결과(y_train) 지정\n",
        "x_train = ipc_df_trn[features]\n",
        "y_train = ipc_df_trn[\"Result\"]\n",
        "\n",
        "print(x_train[:3])\n",
        "print(y_train[:3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2cg5ngeYyFnA"
      },
      "outputs": [],
      "source": [
        "# 검증 데이터(x_val) 및 검증 결과(y_val)지정\n",
        "x_val = ipc_df_val[features]\n",
        "y_val = ipc_df_val[\"Result\"]\n",
        "\n",
        "\n",
        "# y_val의 index 초기화, 초기화 하지 않으면 112 번부터 index가 시작됨\n",
        "y_val.reset_index(drop=True,inplace=True)\n",
        "y_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ0oMMptyFnA"
      },
      "outputs": [],
      "source": [
        "# 선형 회귀 함수를 호출하고 lin이라는 변수로 지정\n",
        "from sklearn.linear_model import LinearRegression\n",
        "lin = LinearRegression()\n",
        "\n",
        "\n",
        "# 훈련 데이터 및 결과(x_train, y_train)를 사용하여 선형 회귀 fitting 진행\n",
        "lin.fit(x_train,y_train)\n",
        "\n",
        "\n",
        "# fitting 된 모델에 검증 데이터(x_val)를 대입하여 예측 결과(y_pred) 도출\n",
        "y_pred = lin.predict(x_val)\n",
        "y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gO_JzOChyFnA"
      },
      "source": [
        "##### 회귀 모델의 성능 평가\n",
        "- 선형 회귀를 포함한 회귀 모델의 성능을 평가하는 지표로 다음과 같은 수치들을 확인함\n",
        "    - MAE(Mean Absolute Error, 편차 절대값의 평균)\n",
        "    - MSE(Mean Squared Error, 편차 제곱의 평균)\n",
        "    - RMSE(Root Mean Squared Error, 편차 제곱의 평균의 제곱근)\n",
        "        - 위 3개 지표는 편차(= 예측 - 실측)을 평가하는 지표\n",
        "        - 편차의 음수/양수 분포로 인한 상쇄 영향을 줄이기 위해 절대값(MAE) 또는 제곱(MSE)을 사용하거나, 다시 제곱으로 인한 과대평가 가능성을 줄이기 위해 제곱근을 적용한 지표(RMSE)\n",
        "    - R-Sqaured(R2)\n",
        "        - 회귀 모델의 성능 평가를 위한 기본적인 지표\n",
        "        - MSE를 분산으로 정규화한 값을 사용\n",
        "        - 일반적으로 0~1 사이에 분포하며, 1에 가까울 수록 완벽한 예측\n",
        "            - 실제 모델에 대해서는 70% 이상만 되어도 꽤 뛰어난 성능의 예측 모듈로 평가함\n",
        "        - 음수도 나올 수 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xddvT3uyFnA"
      },
      "outputs": [],
      "source": [
        "# Skit-learn 라이브러리의 Metric 모듈에서 성능 평가를 위한 함수 호출\n",
        "from sklearn.metrics import mean_absolute_error as MAE, mean_squared_error as MSE, r2_score as R2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# 검증 결과(y_val)와 예측 결과(y_pred)를 사용한 선형 회귀 모델 평가\n",
        "def show_regression_result(y_val, y_pred, N=100):\n",
        "    plt.rc('figure', figsize=(6,4)) # 그래프의 이미지 사이즈를 6 X 4 로 지정\n",
        "    error = y_pred - y_val # y_pred와 y_val의 차이를 편차(error)로 지정\n",
        "    print(\"MAE=\", MAE(y_true=y_val,y_pred=y_pred)) # MAE 계산 및 출력\n",
        "    print(\"RMSE=\", MSE(y_true=y_val,y_pred=y_pred)**0.5) # RMSE 계산 및 출력\n",
        "    print(\"R2=\",R2(y_val, y_pred)) # R-Squared 계산 및 출력\n",
        "    print(\"max error=\", max(abs(error))) # Max Error 계산 및 출력\n",
        "\n",
        "    if N > 0:\n",
        "      plt.plot(y_val[:N], 'r.-', label='y_val')\n",
        "      plt.plot(y_pred[:N], 'b.-', label='y_pred')\n",
        "      plt.legend()\n",
        "      plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9c-EETDxyFnA"
      },
      "outputs": [],
      "source": [
        "show_regression_result(y_val, y_pred, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3oQyIiAyFnB"
      },
      "outputs": [],
      "source": [
        "# 가중치를 Horizontal Bar graph로 확인하는 함수\n",
        "def plot_features_weights(features, weights):\n",
        "    W = pd.DataFrame({'Weights':weights}, index=features) # 가중치 데이터를 df 형태로 저장\n",
        "    W = W.sort_values(by='Weights', ascending=True) # 가중치 데이터를 오름 차순 정렬\n",
        "    W.plot(kind='barh', figsize=(8, 6)) # 가중치 데이터 시각화"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHhckPY4yFnB"
      },
      "outputs": [],
      "source": [
        "print(sorted(lin.coef_,reverse=True)) # 가중치 데이터 출력\n",
        "\n",
        "plot_features_weights(x_train.columns, lin.coef_) # 함수 실행\n",
        "\n",
        "# 예측 결과값(y_pred) 삭제\n",
        "del y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7k1Ph38yFnB"
      },
      "source": [
        "#### 결정 트리 모델(Decision Tree Model, DTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KB7ZjrEyFnB"
      },
      "outputs": [],
      "source": [
        "## 결정 트리를 이용한 이진 분류\n",
        "# 트리의 깊이를 작게 선택하면 과소적합(under fitting)이 발생한다\n",
        "# 트리의 깊이를 깊게 선택하면 과대적합(over fitting)이 발생한다\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor as DTR\n",
        "\n",
        "\n",
        "# 최적 깊이를 찾기 위한 반복적인 fitting 실행\n",
        "score_list = []\n",
        "for depth in range(1,100):\n",
        "    dtr = DTR(max_depth=depth)\n",
        "    dtr.fit(x_train, y_train)\n",
        "    print(depth, dtr.score(x_val, y_val).round(4))\n",
        "    score_list.append(dtr.score(x_val, y_val).round(4))\n",
        "\n",
        "\n",
        "# 확인된 최적 깊이와 그 때 fitting score 확인\n",
        "opt_depth = score_list.index(max(score_list))+1\n",
        "print(f\"Optimized Depth: {opt_depth}, Score: {max(score_list)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cItxHLu_yFnB"
      },
      "outputs": [],
      "source": [
        "# 최적의 깊이를 갖는 트리 모델\n",
        "dtr = DTR(max_depth=opt_depth)\n",
        "dtr.fit(x_train, y_train)\n",
        "y_pred = dtr.predict(x_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JjvowPsyFnB"
      },
      "outputs": [],
      "source": [
        "# 트리 모델 성능 평가\n",
        "show_regression_result(y_val, y_pred, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjnAYgC4yFnB"
      },
      "source": [
        "- R2 값이 음수가 나올 정도로 예측 성능이 좋지 않음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9KVc-CmyFnB"
      },
      "outputs": [],
      "source": [
        "# 트리 모델 특성 중요도 보기\n",
        "# 특성 중요도: 트리를 나눌때 순도를 높이는데(잘 분류하는데) 많이 기여한 정도를 알려준다\n",
        "plot_features_weights(x_train.columns, dtr.feature_importances_)\n",
        "\n",
        "\n",
        "# 예측 결과값(y_pred) 삭제\n",
        "del y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDbC3WnwyFnB"
      },
      "source": [
        "#### 랜덤 포레스트 모델(Random Forest Model, RFM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSeixiLsyFnB"
      },
      "source": [
        "- Random Forest Model은 Decision Forest Model의 성능을 개선한 앙상블 모델임\n",
        "    - 앙상블 모델은 여러 개의 Training Data Set을 생성하여 Fitting을 함으로써 좀 더 정확한 예측을 도출하는 모델임\n",
        "    - 이중 RFM은 이름과 같이 Forest 내에서 Tree를 랜덤하게 추출한 작은 Forest(데이터의 일부만 취한 새로운 Training Set, Weak Data)를 만들어 Fitting을 여러번 진행함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VRujYG1MyFnB"
      },
      "outputs": [],
      "source": [
        "# RFM 호출\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor as RFR\n",
        "rfr = RFR()\n",
        "\n",
        "\n",
        "# Training Set을 통한 RFM fitting 및 예측 결과값(y_pred) 도출\n",
        "rfr.fit(x_train,y_train)\n",
        "y_pred = rfr.predict(x_val)\n",
        "\n",
        "\n",
        "# 데이터 시각화\n",
        "show_regression_result(y_val=y_val,y_pred=y_pred,N=1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zo6-0lJgyFnC"
      },
      "outputs": [],
      "source": [
        "# 랜덤 포레스트 모델의 특성 가중치 보기\n",
        "plot_features_weights(x_val.columns, rfr.feature_importances_)\n",
        "\n",
        "\n",
        "# 예측 결과값(y_pred) 삭제\n",
        "del y_pred"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jud_GYy6yFnC"
      },
      "source": [
        "#### 부스팅 모델(Boosting Model, BM 또는 Boosting Algorithm, BA)\n",
        "- Boosting Model은 Decision Forest Model의 성능을 개선한 앙상블 모델임\n",
        "    - BM은 Forest 내에서 Tree를 순차적으로 추출한 작은 Forest(Weak Data)를 만들어 Fitting을 여러번 진행함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQYhVslayFnC"
      },
      "outputs": [],
      "source": [
        "# Light Gradient BM(LGBM) 모듈 호출\n",
        "from lightgbm import LGBMClassifier, LGBMRegressor\n",
        "LGBMr = LGBMRegressor()\n",
        "\n",
        "\n",
        "# Training Set을 통한 LGBM fitting 및 예측 결과값(y_pred) 도출\n",
        "LGBMr.fit(x_train,y_train)\n",
        "y_pred = LGBMr.predict(x_val)\n",
        "\n",
        "\n",
        "# 시각화\n",
        "show_regression_result(y_val=y_val,y_pred=y_pred,N=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJr8Ve64yFnC"
      },
      "source": [
        "#### 신경망 모델(Neural Network Model, NNM)\n",
        "- 인간의 두뇌가 시각 및 청각 정보를 처리하는 방식을 모방한 인공지능 모델\n",
        "- Training Set의 훈련 데이터(x_train)와 훈련 결과(y_train) 사이의 비선형 관계를 다룰 수 있음\n",
        "    - 최초 모델 공개 당시에는 단순한 선형 관계 에측만 가능하였기에 논리적 한계가 존재하는 이유로 사장되었으나, 하드웨어 스펙의 상승과 더불어 비선형 결합을 통한 예측이 가능해지며 다시 연구되고 있는 분야\n",
        "- 입력층(입력 시퀀스, Input Sequence)과 출력층(출력 시퀀스, Output Sequence) 사이에 몇 가지 층위(은닉층, Hidden layer)를 만들어 각각의 요소(세포, Cell)을 무작위 가중치를 부여하여 연결한 뒤 예측값을 출력하는 모델\n",
        "\n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/NNM.PNG?raw=true\"  width=700 align=\"left\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WgG7WXruyFnC"
      },
      "source": [
        "##### 장단기 메모리(Long Short Term Memory, LSTM)\n",
        "\n",
        "* NNM 중 하나인 순환 신경망 모델(Recurrent Neural Network, RNN)은 아래와 같은 방식으로 input(X_t)에 대한 반복 학습 및 예측 후 output(h_t)가 출력됨\n",
        "    * 반복 과정을 거쳐 상당히 정교한 예측이 가능함\n",
        "    * 단, 장기간 의존(Long-term dependency), 반복하는 과정 중 생성된 최근 n 개의 데이터만을 토대로 학습을 진행하기 때문에 그보다 더 과거에 생성된 중요 데이터들에 대한 학습이 누락된다는 문제점이 존재\n",
        "        * 단순히 n 을 늘려서 해결하는 경우 학습해야 하는 데이터 사이즈가 불필요하게 커져 예측의 정확도를 떨어뜨리는 문제가 있음\n",
        "<img src=\"https://t1.daumcdn.net/cfile/tistory/9901A1415ACB86A021?original\" width=700 align=\"left\">\n",
        "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
        "\n",
        "* 장단기 메모리(LSTM)은 그러한 한계를 보완하여 설계된 모델\n",
        "* LSTM은 Samples - time Steps - Features 구조의 입력이 필요함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9HVJ5mFyFnC"
      },
      "source": [
        "###### Sequence를 n_step의 크기로 나눠서 샘플을 만듦\n",
        "- LSTM 모델은 n 번째 Set의 데이터로 n+1 번째 Set의 결과를 예측하므로 해당 마지막 데이터 샘플은 포함하지 않음\n",
        "\n",
        "<img src=\"https://github.com/imbiotech/skbtML/blob/main/LSTM.png?raw=true\" width=700 align=\"left\">\n",
        "<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>\n",
        "- 단, 현재 데이터로 현재를 예측하는 것이 맞는지는 응용에 따라서 점검해야 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zUrZK6EJyFnC"
      },
      "outputs": [],
      "source": [
        "# numpy 라이브러리를 np라는 이름으로 호출\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Sequence를 n_steps 사이즈로 나누는 함수\n",
        "def split_seq(sequence, n_steps):\n",
        "    x = []\n",
        "    # Sequence가 30, n이 3일 경우, [0, 1, 2] / [1, 2, 3] / [2, 3, 4] / .... / [28, 29, 30] 으로 분류하여 해당 값을 하나의 2차원 배열(array)로 반환함\n",
        "    for i in range(len(sequence) - n_steps):\n",
        "        x.append(sequence[i:i + n_steps])\n",
        "    return np.array(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv4raZyVyFnC"
      },
      "outputs": [],
      "source": [
        "# 위 함수를 사용한 배열 구성의 예시\n",
        "\n",
        "# 0~30까지 숫자를 통해, 순차적으로 3개의 숫자를 포함한 2차원 배열을 만듦\n",
        "# 마지막 데이터 샘플인 [28, 29, 30] 은 포함되지 않음\n",
        "x = np.arange(30).reshape(-1,3)\n",
        "\n",
        "# 10 * 3 크기의 2 차원 배열\n",
        "# x =  [  [ 0,  1,  2],  [ 3,  4,  5],  [ 6,  7,  8],  [ 9, 10, 11],  [12, 13, 14],  [15, 16, 17],  [18, 19, 20],  [21, 22, 23],  [24, 25, 26],  [27, 28, 29]  ]\n",
        "\n",
        "\n",
        "# 해당 2차원 배열을 순차적으로 4개씩 묶어 3차원 배열로 반환, 단 마지막 데이터 제외\n",
        "inp=split_seq(x,4)\n",
        "\n",
        "# 6 * 4 * 3 크기의 3 차원 배열\n",
        "# inp =\n",
        "# [\n",
        "#     배열 1: x의 0~4번 배열을 하나로 묶은 배열\n",
        "#     [ 배열 1-1 [ 0,  1,  2], 배열 1-2 [ 3,  4,  5], 배열 1-3 [ 6,  7,  8], 배열 1-4 [ 9, 10, 11] ],\n",
        "#     배열 2: x의 1~5번 배열을 하나로 묶은 배열\n",
        "#     [ 배열 2-1 [ 3,  4,  5], 배열 2-2 [ 6,  7,  8], 배열 2-3 [ 9, 10, 11], 배열 2-4 [12, 13, 14] ],\n",
        "#     ...\n",
        "#     배열 6: x의 6~9번 배열을 하나로 묶은 배열\n",
        "#     [ 배열 6-1 [15, 16, 17], 배열 6-2 [18, 19, 20], 배열 6-3 [21, 22, 23], 배열 6-4 [24, 25, 26] ]\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5iD6EpryFnC"
      },
      "source": [
        "###### 실제 입력 시퀀스(Input Sequence) 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tysnk8xnyFnC"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터(ipc_df_trn[features])를 n_steps개 씩 나눠서 input sequence를 생성\n",
        "n_steps = 20\n",
        "seq_array = split_seq(ipc_df_trn[features],n_steps)\n",
        "\n",
        "\n",
        "# 만들어진 3차원 배열의 데이터를 출력\n",
        "print(\"Array Shape: \",seq_array.shape) # 90 * 20 * 6 형태의 배열 데이터가 만들어짐\n",
        "seq_array\n",
        "\n",
        "# [\n",
        "#     배열 1: 훈련 데이터의 1~51번째 데이터를 합쳐서 만든 배열\n",
        "#     [\n",
        "#         배열 1-1 [-2.520077  ,  0.77039117, -0.60496917, -0.7866208 , 0.52070595, -0.35542961],\n",
        "#         배열 1-2 [-0.66767217,  0.57187809, -0.60496917,  0.02333417, 0.72791194, -0.2947902 ],\n",
        "#         ...,\n",
        "#         배열 1-49 [-0.06759736,  0.351308  ,  0.09124644,  0.17163578, 0.2246974 ,  0.48846879],\n",
        "#         배열 1-50 [-0.32849945, -0.04571816, -0.60496917, -0.14778308, 0.28389911,  0.80682567]\n",
        "#     ],\n",
        "#     배열 2: 훈련 데이터의 2~52번째 데이터를 합쳐서 만든 배열\n",
        "#     [\n",
        "#         배열 2-1 [-0.66767217,  0.57187809, -0.60496917,  0.02333417, 0.72791194, -0.2947902 ],\n",
        "#         ...,\n",
        "#         배열 2-50 [-0.32849945, -0.04571816, -0.60496917, -0.14778308, 0.28389911,  0.80682567],\n",
        "#         ...,\n",
        "#         배열 60-49 [ 0.1150341 ,  0.17485193,  0.72944408,  0.10318889, -0.51532398,  1.81748243],\n",
        "#         배열 60-50 [-0.01541694,  0.24102295,  1.71574952,  0.02333417, -0.60412654,  1.79221601]\n",
        "#     ]\n",
        "# ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBki0NbwyFnD"
      },
      "source": [
        "#### 회귀 모델(Regression Model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8D6f6qtHyFnD"
      },
      "outputs": [],
      "source": [
        "# 훈련 결과(ipc_df_trn[\"Result\"])로 output sequence를 생성\n",
        "label = [\"Result\"]\n",
        "label_array = np.concatenate([ipc_df_trn[label][n_steps:]])\n",
        "\n",
        "print(\"Array Shape: \",label_array.shape) # 90 * 1 형태의 배열 데이터가 만들어짐\n",
        "label_array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8PWzowyyFnD"
      },
      "outputs": [],
      "source": [
        "# 검증 데이터(ipc_df_val[features])에 대해서도 동일 작업 수행\n",
        "seq_array_val = split_seq(ipc_df_val[features],n_steps)\n",
        "\n",
        "\n",
        "# 만들어진 3차원 배열의 데이터를 출력\n",
        "print(\"Array Shape: \",seq_array_val.shape) # 7 * 20 * 6 형태의 배열 데이터가 만들어짐\n",
        "seq_array_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93HYrBJtyFnD"
      },
      "outputs": [],
      "source": [
        "# 검증 결과(ipc_df_val[\"Result\"])로 output sequence를 생성\n",
        "label_array_val = np.concatenate([ipc_df_val[label][n_steps:]])\n",
        "\n",
        "print(\"Array Shape: \",label_array_val.shape) # 7 * 1 형태의 배열 데이터가 만들어짐\n",
        "label_array_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RByTAVvXyFnD"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "from keras import Sequential\n",
        "from keras.layers import Dropout, LSTM, Dense, Activation\n",
        "\n",
        "nb_features = seq_array.shape[2]\n",
        "nb_out = label_array.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(\n",
        "            LSTM(\n",
        "            input_shape=(n_steps, nb_features), # 스텝 수, 특성 수\n",
        "            units=100                           # 셀의 수\n",
        "            )\n",
        "         )\n",
        "# Sequential 모델의 성능 조정을 위해 알아야 할 것\n",
        "# 1. LSTM 셀의 수(units)\n",
        "# 2. Dropout 비율(Dropout)\n",
        "# 3. LSTM 셀의 활성화 함수(Activation)\n",
        "# 4. Dense 층의 활성화 함수(Activation)\n",
        "# 5. Dense 층의 노드 수(units)\n",
        "# 6. Dense 층의 수(Dense)\n",
        "# 7. 최종 출력 층의 활성화 함수(Activation)\n",
        "# 8. 최종 출력 층의 노드 수(units)\n",
        "\n",
        "\n",
        "\n",
        "model.add(Dense(1)) # 1\n",
        "model.add(Activation(\"linear\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dt_PeAT6yFnD"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BCxNAkEyFnD"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mse'])\n",
        "\n",
        "history = model.fit(seq_array, label_array, epochs=1000, batch_size=50, validation_split=0.1, verbose=2,\n",
        "          callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='min')])\n",
        "\n",
        "print(history.history.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qquIJqaayFnD"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(seq_array_val,verbose=1, batch_size=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFcrhtn-yFnD"
      },
      "outputs": [],
      "source": [
        "y_pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcxSX4jQyFnD"
      },
      "outputs": [],
      "source": [
        "show_regression_result(label_array_val, y_pred,1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXPKbWCWyFnD"
      },
      "source": [
        "---\n",
        "---\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPovdo2HyFnE"
      },
      "source": [
        "## 사례 2. B 제품 수율 예측(케이스 스터디 진행 중, 추후 공유 예정)\n",
        "- 대상 제품 및 공정: B 제품의 a 공정 및 b 공정\n",
        "- 분석 개요: 공정 중 몇 가지 파라미터 p1, p2... 의 변화에 따른 수율 영향성을 평가하고, 수율을 예측"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGpMQlGJyFnE"
      },
      "outputs": [],
      "source": [
        "# 훈련 데이터의 URL 주소 설정\n",
        "process_time_data_address = \"https://raw.githubusercontent.com/imbiotech/skbtML/main/process_time_data.csv\"\n",
        "measure_data_address = \"https://raw.githubusercontent.com/imbiotech/skbtML/main/measure_data.csv\"\n",
        "\n",
        "\n",
        "# 파이썬 코드의 #은 한 줄 주석문을 나타내는 것으로 각 코드의 설명을 기록하기 위해 작성함\n",
        "# 여러 줄을 사용하기 위해 '''''' 방식으로 사용하기도 함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5E6sWJuyFnE"
      },
      "outputs": [],
      "source": [
        "# pandas 라이브러리를 가져옴\n",
        "import pandas\n",
        "\n",
        "\n",
        "# 데이터 URL에 저장된 데이터를 dataframe 형태로 읽어옴\n",
        "process_df = pandas.read_csv(process_time_data_address)\n",
        "measure_df = pandas.read_csv(measure_data_address)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68slE-_kyFnE"
      },
      "source": [
        "\n",
        "- 파이썬에서는 복잡한 과정 없이 한 줄의 코드를 통해 필요한 라이브러리의 기능을 사용할 수 있음\n",
        "\n",
        "- 이름이 길어서 pandas를 매번 쓰기 귀찮을 경우 뒤 쪽에 \"as pd\"를 덧붙이면 pandas 대신 pd만 사용하여도 기능을 사용할 수 있음\n",
        "\n",
        "    - import pandas as pd\n",
        "\n",
        "---\n",
        "\n",
        "- pandas.read_csv()는 pandas 라이브러리 내에 있는 read_csv 함수를 호출하여 사용함\n",
        "    - read_csv 함수는 괄호 안의 주소(data_address)에 있는 csv형태의 데이터를 읽어옴\n",
        "\n",
        "- 이 데이터를 CPU에 \"df\"라는 이름의 변수로 저장하고 이후에는 df만 호출하면 해당 데이터를 불러올 수 있음\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ssSxl3ApyFnE"
      },
      "outputs": [],
      "source": [
        "# process_df의 테이블 정보를 확인하고 제대로 된 데이터가 들어왔는지 확인하기 위해 0~2행을 예시 출력\n",
        "print(process_df.info())\n",
        "process_df[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTGVoyp4yFnE"
      },
      "source": [
        "- df의 자료 이름에 [:3]을 붙이면 3행의 앞까지(0~2행)을 출력함"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuAGGn_3yFnE"
      },
      "source": [
        "\n",
        "- process_df에는 배치 번호 컬럼을 포함한 총 5개의 컬럼이 존재하고, 각 컬럼에 포함된 데이터는 2100개가 존재함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wdmPAH3yFnE"
      },
      "outputs": [],
      "source": [
        "print(measure_df.info())\n",
        "measure_df[:3]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjUyEXyhyFnE"
      },
      "source": [
        "\n",
        "- measure_df에는 시간 컬럼을 포함한 총 7개의 컬럼이 존재하고, 각 컬럼에 포함된 데이터는 n개가 존재함\n",
        "\n",
        "- 기본적으로 raw data를 처음 불러 왔을 때 정제 과정을 거친 상태가 아니라면 실제 머신 러닝 모델 구현 시 정확도를 떨어지게 하는 데이터들이 존재할 수 있음\n",
        "\n",
        "- 이를 실제로 사용하기 위해서는 데이터 전처리(Data Preprocessing) 과정을 거칠 필요가 있음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIU4nhruyFnE"
      },
      "outputs": [],
      "source": [
        "# measure_df에서 \"*T*\"가 포함된 컬럼을 선택하여 float형으로 데이터형을 바꿈\n",
        "# 해당 컬럼에는 온도 데이터가 저장되어 있음\n",
        "measure_df[measure_df.columns[measure_df.columns.str.contains(\"*T*\")]] = measure_df[measure_df.columns[measure_df.columns.str.contains(\"*T*\")]].astype(float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsrGC_2FyFnE"
      },
      "outputs": [],
      "source": [
        "# measure_df의 \"Time\" 컬럼을 datetime 형태로 변환\n",
        "measure_df[\"Time\"] = pandas.to_datetime(measure_df[\"Time\"])\n",
        "\n",
        "# process_df의 \"Start_Time\" 컬럼과 \"End_Time\" 컬럼을 datetime 형태로 변환\n",
        "process_df[\"Start_Time\"] = pandas.to_datetime(process_df[\"Start_Time\"])\n",
        "process_df[\"End_Time\"] = pandas.to_datetime(process_df[\"End_Time\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtwPZdwNyFnE"
      },
      "outputs": [],
      "source": [
        "# make function to find measure_df's \"Time\" between process_df's \"Start_Time\" and \"End_Time\" and make new dataframe named measure_df's name+process_df's index number\n",
        "def find_between_time(process_df,measure_df):\n",
        "    df_list = []\n",
        "    for i in range(len(process_df)):\n",
        "        df_list.append(measure_df[(measure_df[\"Time\"] >= process_df[\"Start_Time\"][i]) &(measure_df[\"Time\"] <= process_df[\"End_Time\"][i])])\n",
        "    return df_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1d8M9ViCyFnF"
      },
      "outputs": [],
      "source": [
        "\n",
        "# find_between_time 함수를 사용하여 process_df의 index 번호에 해당하는 measure_df의 데이터를 찾아서 df_list에 저장\n",
        "df_list = find_between_time(process_df,measure_df)\n",
        "\n",
        "# df_list의 0번째 데이터를 예시로 출력\n",
        "df_list[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nL0bzVbLyFnF"
      },
      "outputs": [],
      "source": [
        "df_list[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zNBuvv29yFnF"
      },
      "outputs": [],
      "source": [
        "# 문자형 숫자 데이터를 숫자형으로 전환하기 위한 함수\n",
        "def StringToInt(x: str):\n",
        "    # 결측값인 <null>을 제외한 나머지 문자형 데이터를 숫자형으로 전환함\n",
        "    if x != \"<null>\":\n",
        "        return float(x)\n",
        "\n",
        "# 온도 관련 데이터를 Celcius에서 Kelvin으로 전환하기 위한 함수\n",
        "def CelciusToKelvin(x: float):\n",
        "    return x+273.15\n",
        "\n",
        "\n",
        "# 일시 관련 데이터를 양식 통일하기 위한 함수 및 해당 함수를 위해 datetime 라이브러리를 불러옴\n",
        "from datetime import datetime\n",
        "def StringToDate(x: str):\n",
        "    if type(x) == str:\n",
        "        if \"/\" in x:\n",
        "            return datetime.strptime(x, \"%m/%d/%Y %H:%M:%S.%f\").strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "        else:\n",
        "            return datetime.strptime(x, \"%Y-%m-%d %H:%M\").strftime(\"%Y-%m-%d %H:%M:%S\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uoMICB6yFnF"
      },
      "outputs": [],
      "source": [
        "# process_df 테이블 업데이트\n",
        "\n",
        "columns = process_df.columns\n",
        "\n",
        "for values in columns:\n",
        "    if \"Time\" in values:\n",
        "        process_df[values] = process_df[values].apply(StringToDate)\n",
        "\n",
        "process_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2Z1KYmgyFnF"
      },
      "outputs": [],
      "source": [
        "# measure_df 테이블 업데이트\n",
        "\n",
        "columns = measure_df.columns\n",
        "\n",
        "for values in columns:\n",
        "    if \"Time\" in values:\n",
        "        measure_df[values] = measure_df[values].apply(StringToDate)\n",
        "        continue\n",
        "    measure_df[values] = measure_df[values].apply(StringToInt)\n",
        "    if \"T\" in values:\n",
        "        measure_df[values] = measure_df[values].apply(CelciusToKelvin)\n",
        "\n",
        "measure_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqXe5k-qyFnF"
      },
      "outputs": [],
      "source": [
        "def FindingDateDiff(x: str, y: str, z: int):\n",
        "    X = datetime.strptime(x,\"%Y-%m-%d %H:%M:%S\")\n",
        "    Y = datetime.strptime(process_df[y][z],\"%Y-%m-%d %H:%M:%S\")\n",
        "    return(X-Y).days\n",
        "\n",
        "measure_df2 = measure_df.copy()\n",
        "measure_df3 = measure_df.copy()\n",
        "\n",
        "\n",
        "# 시작 시간의 맨 첫 번째 데이터 확인\n",
        "measure_df2[\"Time\"]=measure_df2[\"Time\"].apply(FindingDateDiff, args=(\"Start_Time\",0))\n",
        "StartIndex = measure_df2[measure_df2[\"Time\"]>=0].index[0]\n",
        "\n",
        "\n",
        "measure_df3[\"Time\"]=measure_df3[\"Time\"].apply(FindingDateDiff, args=(\"End_Time\",0))\n",
        "EndIndex = measure_df3[measure_df3[\"Time\"]>=0].index[0]\n",
        "\n",
        "measure_df[StartIndex:EndIndex+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FyoXoEMEyFnF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7h2Ux5oCyFnF"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "k_pda_gmp_w-s.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}